{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dokee\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from lib import RedNet_model\n",
    "from lib import RedNet_data\n",
    "from lib.utils import utils\n",
    "from lib.utils.utils import save_ckpt\n",
    "from lib.utils.utils import print_log\n",
    "import imageio\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CARLA(Dataset):\n",
    "    def __init__(self, transform=None, phase_train=True):\n",
    "        self.phase_train = phase_train\n",
    "        self.transform = transform\n",
    "        tmp = pd.read_csv(\"data/run1/train.csv\")\n",
    "        self.train_files = tmp[\"Name\"]\n",
    "        tmp = pd.read_csv(\"data/run1/test.csv\")\n",
    "        self.test_files = tmp[\"Name\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.phase_train:\n",
    "            return len(self.train_files)\n",
    "        else:\n",
    "            return len(self.test_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.phase_train == True:\n",
    "            files = self.train_files\n",
    "        else:\n",
    "            files = self.test_files\n",
    "        label = np.load(\"data/run2/semantic/\" + files[idx] + \".npy\")\n",
    "        depth = np.load(\"data/run2/depth/\" + files[idx] + \".npy\")\n",
    "        image = imageio.v2.imread(\"data/run2/rgb/\" + files[idx] + \".png\", pilmode='RGB')\n",
    "        sample = {'image':image, 'depth': depth, 'label':label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(600, 800, 3)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = imageio.v2.imread(\"data/run1/depth/image_1.png\", pilmode='RGB')\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "image_w = 640\n",
    "image_h = 480\n",
    "batch_size = 5\n",
    "workers = 0\n",
    "lr = 2e-3\n",
    "lr_decay = 0.8\n",
    "epoch_per_decay = 100\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "epochs = 1500\n",
    "start_epoch = 0\n",
    "save_epoch_freq = 5\n",
    "print_freq = 50\n",
    "summary_dir = 'lib/models/model1/summary'\n",
    "ckpt_dir = 'lib/models/model1/'\n",
    "checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dokee\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "train_data = CARLA(transform=transforms.Compose([RedNet_data.scaleNorm(),\n",
    "                                                 RedNet_data.RandomScale((1.0, 1.4)),\n",
    "                                                 RedNet_data.RandomHSV((0.9, 1.1),\n",
    "                                                                       (0.9, 1.1),\n",
    "                                                                       (25, 25)),\n",
    "                                                 RedNet_data.RandomCrop(image_h, image_w),\n",
    "                                                 RedNet_data.RandomFlip(),\n",
    "                                                 RedNet_data.ToTensor(),\n",
    "                                                 RedNet_data.Normalize()]),\n",
    "                   phase_train=True)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=workers, pin_memory=False)\n",
    "num_train = len(train_data)\n",
    "model = RedNet_model.RedNet(pretrained=False)\n",
    "CEL_weighted = utils.CrossEntropyLoss2d()\n",
    "model.train()\n",
    "model.to(device)\n",
    "CEL_weighted.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,\n",
    "                            momentum=momentum, weight_decay=weight_decay)\n",
    "global_step = 0\n",
    "lr_decay_lambda = lambda epoch: lr_decay ** (epoch // epoch_per_decay)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_decay_lambda)\n",
    "\n",
    "writer = SummaryWriter(summary_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dokee\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "C:\\Users\\dokee\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab_test [0 1 2] [0 1 2] [0 1 2] [0]\n",
      "lab_test [0 1 2] [0 1] [0 1] [0]\n",
      "lab_test [0 1] [0 1] [0 1] [0]\n",
      "lab_test [0 1] [0 1] [0 1] [0]\n",
      "lab_test [0 1] [0 1] [0 1] [0]\n",
      "torch.Size([5, 3, 480, 640]) torch.Size([5, 1, 480, 640])\n",
      "[False  True] tensor(1338.1805, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[False  True] tensor(4102.8652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[False  True] tensor(3663.0205, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[False  True] tensor(6831.5376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[False] tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_22148\\3368454730.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mpred_scales\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcheckpoint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCEL_weighted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpred_scales\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_scales\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[0mlocal_count\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    361\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 363\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    365\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    173\u001B[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m def grad(\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(int(start_epoch), epochs):\n",
    "\n",
    "    scheduler.step(epoch)\n",
    "    local_count = 0\n",
    "    last_count = 0\n",
    "    end_time = time.time()\n",
    "    if epoch % save_epoch_freq == 0 and epoch != start_epoch:\n",
    "        save_ckpt( ckpt_dir, model, optimizer, global_step, epoch,\n",
    "                  local_count, num_train)\n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        image = sample['image'].to(device)\n",
    "        depth = sample['depth'].to(device)\n",
    "        print(image.shape, depth.shape)\n",
    "        target_scales = [sample[s].to(device) for s in ['label', 'label2', 'label3', 'label4', 'label5']]\n",
    "        optimizer.zero_grad()\n",
    "        pred_scales = model(image, depth, checkpoint)\n",
    "        loss = CEL_weighted(pred_scales, target_scales)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        local_count += image.data.shape[0]\n",
    "        global_step += 1\n",
    "        if global_step % print_freq == 0 or global_step == 1:\n",
    "\n",
    "            time_inter = time.time() - end_time\n",
    "            count_inter = local_count - last_count\n",
    "            print_log(global_step, epoch, local_count, count_inter,\n",
    "                      num_train, loss, time_inter)\n",
    "            end_time = time.time()\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                writer.add_histogram(name, param.clone().cpu().data.numpy(), global_step, bins='doane')\n",
    "            grid_image = make_grid(image[:3].clone().cpu().data, 3, normalize=True)\n",
    "            writer.add_image('image', grid_image, global_step)\n",
    "            grid_image = make_grid(depth[:3].clone().cpu().data, 3, normalize=True)\n",
    "            writer.add_image('depth', grid_image, global_step)\n",
    "            grid_image = make_grid(utils.color_label(torch.max(pred_scales[0][:3], 1)[1] + 1), 3, normalize=False,\n",
    "                                   range=(0, 255))\n",
    "            writer.add_image('Predicted label', grid_image, global_step)\n",
    "            grid_image = make_grid(utils.color_label(target_scales[0][:3]), 3, normalize=False, range=(0, 255))\n",
    "            writer.add_image('Groundtruth label', grid_image, global_step)\n",
    "            writer.add_scalar('CrossEntropyLoss', loss.data, global_step=global_step)\n",
    "            writer.add_scalar('Learning rate', scheduler.get_lr()[0], global_step=global_step)\n",
    "            last_count = local_count\n",
    "\n",
    "save_ckpt(ckpt_dir, model, optimizer, global_step, epochs,\n",
    "          0, num_train)\n",
    "\n",
    "print(\"Training completed \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}